{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We use the COMPAS dataset here, which is a subset of all the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One line per import\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = # TODO PATH TO DATA FROM https://github.com/stanford-policylab/recidivism-predictions\n",
    "\n",
    "broward_clean_df = pd.read_csv(f'{datadir}/individuals/broward_clean_fixed.csv')\n",
    "compas_scores_df = pd.read_csv(f'{datadir}/individuals/compas_scores.csv')\n",
    "compas_vignettes_df = pd.read_csv(f'{datadir}/individuals/compas_vignettes.csv')\n",
    "\n",
    "response_df = pd.read_csv(f'{datadir}/surveys/df_response.csv')\n",
    "user_df = pd.read_csv(f'{datadir}/surveys/df_user.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "Note that \"users\" correspond to Mechanical Turk participants, and \"individuals\" correspond to the defendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"dummy\" individuals\n",
    "### The top 2 \"individuals\" are dummy indiciators, so we remove them\n",
    "### response_df[['individual_id']].value_counts()\n",
    "\n",
    "bad_ids = ['dummy1', 'dummy2']\n",
    "response_df = response_df.query('individual_id not in @bad_ids')\n",
    "\n",
    "# Fix Datatypes\n",
    "response_df = response_df.astype({'individual_id': int})\n",
    "broward_clean_df = broward_clean_df.astype({'id': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broward_clean_df.loc[broward_clean_df['id'].isin(response_df.individual_id.values)]['two_year_recid'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to cases of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the cases where the user group is `vignette` (indicating that no feedback was given) as opposed to `feedback_vignette`, where feedback is given after each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_df.user_group.value_counts()\n",
    "response_df = response_df.query(\"user_group != 'feedback_vignette'\")\n",
    "response_df = response_df.drop('user_group', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to filter out any users that did not complete the exercise, or who recieved feedback during the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.query(\"user_group != 'feedback_vignette' & exit_status == 'submitted'\")\n",
    "user_df.drop(['user_group', 'exit_status'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three duplicate users (who completed 2 assignments each), so we confirm that their information is stable across assignments, and remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the features we wish to keep: Clarify these are for the USERS not the Defendants\n",
    "user_feat_remap = {\n",
    "    'user_id': 'user_id', \n",
    "    'age': 'user_age',\n",
    "    'gender': 'user_gender',\n",
    "    'degree': 'user_degree'\n",
    "}\n",
    "\n",
    "user_features = [v for k, v in user_feat_remap.items()]\n",
    "\n",
    "# Rename and drop features we do not need\n",
    "user_df = user_df.rename(columns = user_feat_remap)\n",
    "user_df = user_df[user_features]\n",
    "\n",
    "# Remove duplicate users\n",
    "user_df = user_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Features and Predictions\n",
    "\n",
    "Here we use the features from the `broward_clean_df`, which are the same as those in the compas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPAS Risk Factors\n",
    "broward_features = [\n",
    "    # Defendant id and demographics\n",
    "    'id', 'race', 'sex', 'age', \n",
    "    # Criminal history\n",
    "    'juv_fel_count', 'juv_misd_count', 'priors_count', \n",
    "    # Charge identifier and degree\n",
    "    'charge_id', 'charge_degree (misd/fel)'\n",
    "]\n",
    "\n",
    "# Features of user response\n",
    "response_features = [\n",
    "    'user_id', 'individual_id', 'predicted_decision', 'leave_time', 'enter_time'\n",
    "]\n",
    "response_df = response_df[response_features]\n",
    "\n",
    "# Merge risk factors (\"features\") with the user responses\n",
    "response_with_features_df = response_df.merge(\n",
    "    broward_clean_df[broward_features],\n",
    "    left_on='individual_id', right_on='id')\n",
    "\n",
    "# Drop the extraneous \"id\" variable\n",
    "response_with_features_df = response_with_features_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge User information\n",
    "\n",
    "We also pull in the user information, for down-stream analysis.  Note that because we restrict to users that have `exit_status == submitted`, this results in a small reduction in the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df = response_with_features_df.merge(user_df, on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our framework is designed to work with binary decisions, rather than predicted probabilities, so we threshold the probabilities provided by Mechanical Turk participants into binary decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df['outcome'] = np.where(response_with_features_df['predicted_decision'] > 50, 1, 0)\n",
    "del response_with_features_df['predicted_decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also construct a feature for the time spent in the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df['time_deciding'] = \\\n",
    "    response_with_features_df['leave_time'] - response_with_features_df['enter_time']\n",
    "\n",
    "del response_with_features_df['leave_time']\n",
    "del response_with_features_df['enter_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct one-hot encodings of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We derived the race mapping from the following\n",
    "# Get an example of each racial category\n",
    "race_categories_df = response_with_features_df[['individual_id','race']].drop_duplicates(subset='race')\n",
    "\n",
    "# Merge with the compas scores to get a lookup\n",
    "race_categories_df \\\n",
    "    = race_categories_df.merge(compas_scores_df[['id','race']], left_on='individual_id', right_on='id')\n",
    "race_x = race_categories_df.race_x.values\n",
    "race_y = race_categories_df.race_y.values\n",
    "race_dict = dict()\n",
    "for i in range(len(race_x)):\n",
    "    race_dict[f'race_{race_x[i]}'] = f\"race_{race_y[i].lower().replace(' ', '_')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies for the following - we handle charge id below manually\n",
    "cat_feats = ['user_gender', 'user_degree', 'race'] #, 'charge_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df = pd.get_dummies(response_with_features_df, columns = cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'user_gender_f': 'user_gender_female',\n",
    "    'user_gender_m': 'user_gender_male',\n",
    "    'user_gender_o': 'user_gender_other',\n",
    "    'user_degree_Associate degree': 'user_degree_associate',\n",
    "    'user_degree_Bachelor degree': 'user_degree_bachelor',\n",
    "    'user_degree_Master degree': 'user_degree_master_degree',\n",
    "    'user_degree_Doctoral degree': 'user_degree_doctoral',\n",
    "    'user_degree_High school': 'user_degree_high_school',\n",
    "    'user_degree_Middle school': 'user_degree_middle_school',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict.update(race_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df.rename(columns = rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we get an example of each charge ID\n",
    "charge_categories_df \\\n",
    "    = response_with_features_df[['individual_id','charge_id']].drop_duplicates(subset='charge_id')\n",
    "\n",
    "# Then, we map this to names in the compas vignettes\n",
    "charge_categories_df = charge_categories_df.merge(compas_vignettes_df[['id','charge_name']], \\\n",
    "                                                  left_on='individual_id', right_on='id')\n",
    "charge_ids = charge_categories_df.charge_id.values\n",
    "charge_names = charge_categories_df.charge_name.values\n",
    "\n",
    "# We then construct a dictionary to lookup charge names, and reformat them\n",
    "charge_dict = dict()\n",
    "for i in range(len(charge_ids)):\n",
    "    charge_dict[charge_ids[i]] = charge_names[i]\n",
    "charge_dict_rev = dict()\n",
    "for i in charge_dict:\n",
    "    charge_col_name = 'charge_' + charge_dict[i].lower().replace(' ', '_')\n",
    "    if charge_col_name in charge_dict_rev:\n",
    "        charge_dict_rev[charge_col_name].add(i)\n",
    "    else:\n",
    "        charge_dict_rev[charge_col_name] = {i}\n",
    "        \n",
    "# Finally, we construct one-hot encodings manually, based on these names\n",
    "for charge in charge_dict_rev:\n",
    "    response_with_features_df[charge] \\\n",
    "        = np.where(response_with_features_df['charge_id'].isin(charge_dict_rev[charge]), 1, 0)\n",
    "del response_with_features_df['charge_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_with_features_df.to_csv(f'{datadir}/compas_no_feedback_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
